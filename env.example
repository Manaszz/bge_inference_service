# Service
SERVICE_NAME=bge-inference-service
HOST=0.0.0.0
PORT=8011
LOG_LEVEL=INFO

# Models
EMBEDDING_MODEL_NAME=BAAI/bge-m3
RERANKER_MODEL_NAME=BAAI/bge-reranker-v2-m3
# BGE-M3 dense embedding dimensionality
EMBEDDING_SIZE=1024

# Runtime
DEVICE=cuda
USE_FP16=true

# Sparse mapping (must be consistent between indexing and querying)
# - hash: deterministic SHA256 hashing into a fixed index space
# - tokenizer: tokenizer IDs (0 collisions), requires extra RAM
# NOTE: Main project uses 'tokenizer', so we default to it here too.
SPARSE_TOKEN_MAPPING=tokenizer
SPARSE_INDEX_SPACE=1048576
TOKENIZER_TRUST_REMOTE_CODE=true

# Limits
MAX_TEXT_CHARS=10000
MAX_BATCH_SIZE=64
INFERENCE_BATCH_SIZE=256

# Microbatching (batch across concurrent HTTP requests)
# Set true to queue + combine requests into fewer `encode()` calls.
EMBEDDING_MICROBATCH_ENABLED=false
# Window to collect requests (higher -> better batching, higher latency)
EMBEDDING_MICROBATCH_MAX_WAIT_MS=10
# Cap on total number of texts in one microbatch
EMBEDDING_MICROBATCH_MAX_BATCH_TEXTS=256
# Max items waiting in queue (backpressure if full)
EMBEDDING_MICROBATCH_QUEUE_MAXSIZE=2048

# OpenAI-compatible aliases
OPENAI_DEFAULT_MODEL_ALIAS=bge-m3
