# Service
SERVICE_NAME=bge-inference-service
HOST=0.0.0.0
PORT=8011
LOG_LEVEL=INFO

# Models
EMBEDDING_MODEL_NAME=BAAI/bge-m3
RERANKER_MODEL_NAME=BAAI/bge-reranker-v2-m3
# BGE-M3 dense embedding dimensionality
EMBEDDING_SIZE=1024

# Runtime
DEVICE=cuda
USE_FP16=true

# Sparse mapping (must be consistent between indexing and querying)
# - hash: deterministic SHA256 hashing into a fixed index space
# - tokenizer: tokenizer IDs (0 collisions), requires extra RAM
# NOTE: Main project uses 'tokenizer', so we default to it here too.
SPARSE_TOKEN_MAPPING=tokenizer
SPARSE_INDEX_SPACE=1048576
TOKENIZER_TRUST_REMOTE_CODE=true

# Limits
MAX_TEXT_CHARS=10000
MAX_BATCH_SIZE=64

# OpenAI-compatible aliases
OPENAI_DEFAULT_MODEL_ALIAS=bge-m3
